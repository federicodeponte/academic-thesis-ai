# OpenDraft - Logging System

This directory contains log files generated by the OpenDraft system.

## Log Files

### `opendraft.log`
**Main log file** - Contains all log messages (DEBUG, INFO, WARNING, ERROR, CRITICAL).

**Rotation**: Automatically rotates when file reaches 10MB, keeps 5 backup files.

**Format**:
```
2025-11-17 18:51:53 | utils.deduplicate_citations    | INFO     | Starting deduplication...
2025-11-17 18:51:54 | utils.scrape_citation_titles   | WARNING  | Timeout while scraping title
2025-11-17 18:51:55 | utils.citation_compiler        | ERROR    | Citation cite_042 not found
```

**Contents**:
- Citation research progress
- Web scraping operations
- Deduplication analysis
- PDF generation steps
- API call statistics
- Performance metrics

### `errors.log`
**Error-only log** - Contains ERROR and CRITICAL messages only.

**Rotation**: Automatically rotates when file reaches 10MB, keeps 5 backup files.

**Use cases**:
- Quick error review without INFO/DEBUG noise
- Production monitoring
- Error rate analysis
- Debugging failed thesis generations

## Log Levels

| Level | Purpose | Example |
|-------|---------|---------|
| **DEBUG** | Detailed execution info | `DEBUG: Query classified as 'academic' with confidence 0.87` |
| **INFO** | Normal operation | `INFO: Scraped 23/25 titles successfully (92%)` |
| **WARNING** | Potential issues | `WARNING: API rate limit approaching (45/50 requests)` |
| **ERROR** | Operation failed | `ERROR: Failed to scrape title from bcg.com: timeout` |
| **CRITICAL** | System failure | `CRITICAL: Citation database corrupted, aborting` |

## Usage in Code

### Basic logging:
```python
from utils.logging_config import get_logger

logger = get_logger(__name__)

logger.info("Starting citation research...")
logger.warning("API rate limit approaching")
logger.error("Failed to scrape title", exc_info=True)
```

### Module-specific logger:
```python
# In utils/deduplicate_citations.py
logger = get_logger(__name__)  # Creates logger named 'utils.deduplicate_citations'

logger.debug(f"Comparing citations: {cite1['id']} vs {cite2['id']}")
logger.info(f"Removed {removed} duplicates")
```

### With exception info:
```python
try:
    scrape_title(url)
except Exception as e:
    logger.error(f"Failed to scrape {url}", exc_info=True)  # Includes full traceback
```

## Configuration

### Default (Production):
```python
from utils.logging_config import setup_logging

# Console: INFO+ (color-coded)
# Files: DEBUG+ (all messages)
setup_logging(level=logging.INFO)
```

### Development (Verbose):
```python
# Console: DEBUG+ (verbose)
# Files: DEBUG+ (all messages)
setup_logging(level=logging.DEBUG)
```

### Production (Quiet):
```python
# Console: WARNING+ (errors only)
# Files: DEBUG+ (full logging)
setup_logging(level=logging.WARNING)
```

### Testing (No files):
```python
# Console: DEBUG+ (verbose)
# Files: disabled
setup_logging(level=logging.DEBUG, file_output=False)
```

## Log Rotation

**Automatic rotation** occurs when log files reach 10MB:

```
opendraft.log           # Current log
opendraft.log.1         # Most recent backup
opendraft.log.2         # 2nd most recent
opendraft.log.3         # 3rd most recent
opendraft.log.4         # 4th most recent
opendraft.log.5         # Oldest backup
```

Oldest backup (`.5`) is deleted when new rotation occurs.

## Monitoring

### Check for errors:
```bash
grep "ERROR" logs/errors.log
```

### Check recent activity:
```bash
tail -f logs/opendraft.log
```

### Count errors by type:
```bash
grep "ERROR" logs/opendraft.log | cut -d'|' -f2 | sort | uniq -c
```

### Find failed scrapes:
```bash
grep "Failed to scrape" logs/opendraft.log | wc -l
```

## Maintenance

### Clean old logs:
```bash
# Remove all backups
rm logs/*.log.[0-9]*

# Remove all logs (fresh start)
rm logs/*.log*
```

### Archive logs:
```bash
# Compress and archive
tar -czf logs_backup_$(date +%Y%m%d).tar.gz logs/
mv logs_backup_*.tar.gz archives/
rm logs/*.log*
```

## Best Practices

### ✅ DO:
- Use appropriate log levels (DEBUG for development, INFO for production)
- Include context in log messages (citation IDs, file paths, API names)
- Use `exc_info=True` for exception logging
- Log operation start/end for performance tracking
- Log API statistics (success rate, response times)

### ❌ DON'T:
- Log sensitive data (API keys, user credentials)
- Log massive data structures (entire citation databases)
- Use logging for user-facing output (use print() or return values)
- Set DEBUG level in production (performance impact)
- Commit log files to git (.gitignore prevents this)

## Performance Impact

**Console logging**: Negligible (<1ms per log message)
**File logging**: Minimal (<2ms per log message with rotation)
**DEBUG level**: Moderate (10-20% overhead with verbose logging)

**Recommendation**: Use INFO level for production, DEBUG for development.

## Troubleshooting

### No logs appearing:
```python
# Check if logging is initialized
import logging
logging.getLogger().handlers  # Should show handlers

# Reinitialize if needed
from utils.logging_config import setup_logging
setup_logging(level=logging.DEBUG, console_output=True, file_output=True)
```

### Logs not rotating:
```bash
# Check file permissions
ls -lh logs/

# Should be writable by current user
chmod 644 logs/*.log
```

### Duplicate log messages:
```python
# Remove duplicate handlers
import logging
logging.getLogger().handlers.clear()

# Reinitialize
from utils.logging_config import setup_logging
setup_logging()
```

## Integration Examples

### Thesis generation script:
```python
from utils.logging_config import get_logger

logger = get_logger(__name__)

def main():
    logger.info("Starting thesis generation...")

    try:
        scout_result = research_citations(...)
        logger.info(f"Scout found {scout_result['count']} citations")

        dedup_citations, stats = deduplicate_citations(...)
        logger.info(f"Removed {stats['removed_count']} duplicates")

        pdf_path = export_pdf(...)
        logger.info(f"PDF exported: {pdf_path}")

    except Exception as e:
        logger.error("Thesis generation failed", exc_info=True)
        raise
```

### Web scraping utility:
```python
from utils.logging_config import get_logger

logger = get_logger(__name__)

def scrape_title(url: str) -> Optional[str]:
    logger.debug(f"Scraping title from {url}")

    try:
        response = requests.get(url, timeout=10)
        title = extract_title(response.text)
        logger.info(f"Scraped title: {title[:50]}...")
        return title

    except requests.Timeout:
        logger.warning(f"Timeout while scraping {url}")
        return None

    except Exception as e:
        logger.error(f"Failed to scrape {url}", exc_info=True)
        return None
```

---

**Last Updated**: 2025-11-17 (Day 6 - Logging Infrastructure)
